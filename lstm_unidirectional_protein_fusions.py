# -*- coding: utf-8 -*-
"""lsmt_network_gridsearchCV.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/17XS-yeUPJkzOEl2pOP1O_p4YMogozrrj

# LSTM for predicting if proteins fusions are involved in cancer presence

**Idea**: Gene fusions are the result of the juxtaposition of two non consecutive DNA regions (usually two
distinct genes). This type of alteration can result as a driver of tumor processes (Oncogenic class) or
generate proteins without any oncogenic effect (NotOncogenic class).

We will train a LSTM network to achieve this goal, because recurrent networks work well with data series, as in this case.

## Libraries we will use
The network is trained by using Keras, an high-level collection of APIs based on Tensorflow backend. Since a certain tensorflow version, keras is directly integrad into it, so we can directly call APIs using tensorflow. Then we will use pandas and numpy, a library for managing and manipulating dataframes and a library for managing matrix and arrays in a suitable way.
"""

import tensorflow as tf
from sklearn.metrics import confusion_matrix, classification_report
import pandas as pd
import numpy as np
import utils as utl
from collections import Counter
from keras.wrappers.scikit_learn import KerasClassifier
from sklearn.model_selection import GridSearchCV
from sklearn.model_selection import train_test_split

"""## Model architecture
In order to train our network we ave to define how it has to work and how it is built.
Our network is composed by:


*   An initial embedding layer, used for learning how discrete objects (such as protein fusion) can be mapped to continous values
*   A series of lstm layers followed by a dropout layer for each of them
*   A final dense layer, used to classify the observations we have provided as input.

We have set RMSprop as network optimizer, since it is indicated as the best in training LSTM networks. In order to decrease the loss of the network is necessary to indicate a loss function. We have seen that mean_square_loss works well in this context so we decided to use it.

Parameters to tune in order to reach best results are:
* Dropout keep probability, i.e. size of edges to cut in training the network
* Learning rate, i.e. how much large has to be the movement onto the surface we want to reach minimum for
"""

class LstmKerasModel:
    """
    This class wraps an LSTM architecture using keras API
    It is similar to class LstmModel, but that's implemented by using
    pure tensorflow APIs
    """

    def __init__(self, vocabulary, hidden_size=512, batch_size=20, n_steps=10, n_epochs=50):
        """
        net is the model built by keras APIs at object creation
        """
        self.net = tf.keras.Sequential()
        self.batch_size = batch_size
        self.n_steps = n_steps
        self.n_epochs = n_epochs
        self.vocabulary = vocabulary
        self.hidden_size = hidden_size

    def build_architecture(self, keep_prob_=0.2, learning_rate=0.00125, optimizer='RMSprop', activation='sigmoid'):
        """
        Here it's built the network architecture
        """
        self.net.add(tf.keras.layers.Embedding(self.vocabulary, self.hidden_size))

        # Add LSTM layers
        # This layer is not optimized for GPU. When a GPU is available replace it
        # with tf.compat.v1.keras.layers.CuDNNLSTM
        self.net.add(tf.compat.v1.keras.layers.CuDNNLSTM(128, return_sequences=True))
        
        # Add a dropout layer at the end of lstm layer
        self.net.add(tf.keras.layers.Dropout(rate=keep_prob_))
        
        self.net.add(tf.compat.v1.keras.layers.CuDNNLSTM(64, return_sequences=True))

        # Add a dropout layer at the end of lstm layer
        self.net.add(tf.keras.layers.Dropout(rate=keep_prob_))
            
        self.net.add(tf.compat.v1.keras.layers.CuDNNLSTM(32, return_sequences=False))

        # Add a dropout layer at the end of lstm layer
        self.net.add(tf.keras.layers.Dropout(rate=keep_prob_))

        # self.net.add(tf.keras.layers.Dense(16, activation='relu'))

        # Add a dense layer at the end in order to classify the samples
        self.net.add(tf.keras.layers.Dense(1, activation=activation))

        # Build the optimizer
        self.build_optimizer(learning_rate, optimizer)
        
        return self.net

    def build_optimizer(self, lr, optmizer):
        """
        Create the Loss function and Optimizer
        """ 
        opt = tf.keras.optimizers.RMSprop(name='RMSprop', learning_rate=lr)
        
        # Create an optmizer according to optmizer name passed
        if optimizer == 'RMSprop':
          opt = tf.keras.optimizers.RMSprop(name='RMSprop', learning_rate=lr)
        elif optmizer == 'SGD':
          opt = tf.keras.optimizers.SGD(name='SGD', learning_rate=lr)
        elif optmizer == 'Adagrad':
          opt = tf.keras.optimizers.Adagrad(name='Adagrad', learning_rate=lr)
        elif optmizer == 'Adadelta':
          opt = tf.keras.optimizers.Adadelta(name='Adadelta', learning_rate=lr)
        elif optmizer == 'Adam':
          opt = tf.keras.optimizers.Adam(name='Adam', learning_rate=lr)
        elif optmizer == 'Adamax':
          opt = tf.keras.optimizers.Adamax(name='Adamax', learning_rate=lr)
        elif optmizer == 'Nadam':
          opt = tf.keras.optimizers.Nadam(name='Nadam', learning_rate=lr)
          
        # Compile model
        self.net.compile(loss=tf.keras.losses.mean_squared_error, optimizer=opt,
                         metrics=['accuracy'])

    def get_batches(self, x, y, batch_size):
        """
        Batch Generator for Training
        :param x: Input array of x data
        :param y: Input array of y data
        :param batch_size: Input int, size of batch
        :return: generator that returns a tuple of our x batch and y batch
        """
        n_batches = len(x) // batch_size
        x, y = x[:n_batches * batch_size], y[:n_batches * batch_size]

        while True:
            for ii in range(0, len(x), batch_size):
                # TODO Here we will put the encoding + padding function.
                #  It performs the transformations before emitting batches
                yield x[ii:ii + batch_size], y[ii:ii + batch_size]

    def train(self, x_train, y_train, x_val, y_val):
        """
        Train the network
        :param x_train:
        :param y_train:
        :param x_val:
        :param y_val:
        :return:
        """
        # Print net summary
        print(self.net.summary())
        
        # Create a callback for saving best found model
        checkpointer = tf.keras.callbacks.ModelCheckpoint(
            filepath='/tmp/weights.hdf5', verbose=0, 
            save_best_only=True)
        
        # Early stopping callback function
#         early_stop = tf.keras.callbacks.EarlyStopping(monitor='val_loss', 
#                                                    min_delta=0, 
#                                                    patience=0, 
#                                                    verbose=0, 
#                                                    mode='auto', 
#                                                    baseline=None, 
#                                                    restore_best_weights=False)

        # Fits the model on data yielded batch-by-batch by a Python generator.
        self.net.fit_generator(self.get_batches(x_train, y_train, self.batch_size),
                               len(x_train) // (self.batch_size * self.n_steps), 
                               self.n_epochs,
                               validation_data=(x_val, y_val), 
                               callbacks=[checkpointer])

    def test(self, x_test, y_test):
        """
        Test data using the built model
        :param x_test:
        :param y_test:
        :return:
        """
        predict_classes = self.net.predict_classes(x_test, len(x_test), verbose=1)
        conf_matr = confusion_matrix(y_test, predict_classes)
        print('Confusion matrix of test:\n', conf_matr)
        print('Classification report:\n', classification_report(y_test, predict_classes))

def create_model(vocab_size, hidden_size, batch_size, num_steps, n_epochs, dropout_rate, learning_rate, optimizer, activation):
  """
  This function is used by gridSearchCV for building a model and training it
  """
  model = LstmKerasModel(vocab_size, hidden_size, batch_size, num_steps, n_epochs)
  net = model.build_architecture(dropout_rate, learning_rate, optimizer, activation)
  return net

"""# Read and preprocess data"""

# Read data from CSV into dataframe
data = pd.read_csv('bin_all_header.csv')
features = data.protein.values
labels = data.label.map({'C': 1,  'N': 0}).values

# Define the list of all words and create a dictionary
full_lexicon = " ".join(features).split()
vocab_to_int, int_to_vocab = utl.create_lookup_tables(full_lexicon)

# Check proteins lenghts and statistics
proteins_lens = Counter([len(x) for x in features])
print("Zero-length proteins: {}".format(proteins_lens[0]))
print("Maximum protein length: {}".format(max(proteins_lens)))
print("Average protein length: {}".format(np.mean([len(x) for x in features])))

# Encode features and labels
# No need to encode labels, since they has been encoded during the data loading
proteins = utl.encode_ST_messages(features, vocab_to_int)

# Then I have to pad proteins, before applying one-hot encoding
proteins = utl.zero_pad_messages(proteins, seq_len=max(proteins_lens))

"""# Create training, validation and testing sets
In order to train a network we need to split the entire dataset into subsets, and some of them are used as never seen data. We must not use the whole dataset both for training and for testing, because we could overfit the network. We have to simulate to be in a real context, i.e. using the model to predict new available data in such a moment. To fit this rule we split data in two different datasets:
* Training dataset: used to train the network, it represents already available data
* Testing dataset: used when the model has been trained, it tests how much well the network works

For this analysis, we have decided to use a fraction of splitting equals to 0.80, i.e. the training set will be equals to 80% of the original one, while testing set will contain the remaining 20% of data.
"""

# New code: since we use now a GridSearchCV we don't need anymore of a validation set, but just a training and a testing set
train_x, test_x, train_y, test_y = train_test_split(proteins, labels, test_size=0.2, random_state=72)

print("Data Set Size")
print("Train set: \t\t{}".format(train_x.shape),
      "\nTest set: \t\t{}".format(test_x.shape))

"""## Model parameters"""

# Define Inputs and Hyperparameters
lstm_sizes = [128, 64, 32]

# add one for padding
vocab_size = [len(vocab_to_int) + 1]

# Number of epochs
n_epochs = [10]

# Size of batches, i.e. subgroups of images used to train the network
batch_size = [20]

# Learning rate for gradient descent optimizer
# learning_rate = [0.001, 0.0125, 0.01, 0.1, 0.2, 0.3, 0.015]
learning_rate = [0.0125]

# Dropout rates
# dropout_rate = [0.0, 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9]
dropout_rate = [0.2]

# Number of steps for training
num_steps = [10]

# Size for embedding layer
hidden_size = [512]

# Activation functions list
# activation = ['softmax', 'softplus', 'softsign', 'relu', 'tanh', 'sigmoid', 'hard_sigmoid', 'linear']
activation = ['sigmoid']

# Optmizers
# optimizer = ['SGD', 'RMSprop', 'Adagrad', 'Adadelta', 'Adam', 'Adamax', 'Nadam']
optimizer = ['RMSprop']

"""# Let's do training!"""

# Create, train and test the model using pure Keras APIs
# model = LstmKerasModel(vocab_size, hidden_size, batch_size, num_steps, n_epochs)
# model.build_architecture(dropout_rate, learning_rate)
# model.train(train_x, train_y, val_x, val_y)

# Create the model
model = KerasClassifier(build_fn=create_model, epochs=n_epochs[0], batch_size=batch_size[0], verbose=1)

# Define parameters dictionary
# vocab_size, hidden_size, batch_size, num_steps, n_epochs, dropout_rate, learning_rate
param_grid = dict(optimizer=optimizer, 
                  activation=activation, 
                  vocab_size=vocab_size, 
                  hidden_size=hidden_size,
                  batch_size=batch_size,
                  num_steps=num_steps,
                  n_epochs=n_epochs,
                  dropout_rate=dropout_rate,
                  learning_rate=learning_rate)
# Build the grid
grid = GridSearchCV(estimator=model, param_grid=param_grid, n_jobs=1)

# Fit the model
grid_result = grid.fit(train_x, train_y)

# Summarize results
print("Best: %f using %s" % (grid_result.best_score_, grid_result.best_params_))
means = grid_result.cv_results_['mean_test_score']
stds = grid_result.cv_results_['std_test_score']
params = grid_result.cv_results_['params']
for mean, stdev, param in zip(means, stds, params):
    print("%f (%f) with: %r" % (mean, stdev, param))

# Take the best model found
best_model = grid.best_estimator_

"""# Let's test the model!
Once got the best model using provided parameters we have to see how it classifies our testing data, i.e. data never seen by model.
"""

predicted_classes = best_model.predict(test_x)
conf_matr = confusion_matrix(test_y, predicted_classes)
print('Confusion matrix of test:\n', conf_matr)
print('Classification report:\n', classification_report(test_y, predicted_classes))
