import tensorflow as tf
import datetime, os
from sklearn.metrics import confusion_matrix, classification_report
from tensorflow import keras
from tensorflow.keras import backend as K
import pandas as pd
import numpy as np
import utils as utl
import matplotlib
from collections import Counter
from sklearn.model_selection import train_test_split, StratifiedKFold
from model import model
import matplotlib.pyplot as plt
# %load_ext tensorboard


def main():
    # Training set: first four proteins bins
    features, labels = read_data('../data/bio_translated_training.csv')
    # Define the list of all words and create a dictionary
    full_lexicon = " ".join(features).split()
    vocab_to_int, int_to_vocab = utl.create_lookup_tables(full_lexicon)
    print("Vocabulary size: {}".format(len(vocab_to_int)))
    print(vocab_to_int)
    
    # Check proteins lenghts and statistics
    proteins_lens = Counter([len(x) for x in features])
    print("Zero-length proteins: {}".format(proteins_lens[0]))
    print("Maximum protein length: {}".format(max(proteins_lens)))
    print("Average protein length: {}".format(np.mean([len(x) for x in features])))
    
    # Encode features and labels
    # No need to encode labels, since they has been encoded during the data loading
    proteins = utl.encode_ST_messages(features, vocab_to_int)
    # Then I have to pad proteins, before applying one-hot encoding
    proteins = utl.zero_pad_messages(proteins, seq_len=max(proteins_lens))

    X_train, y_train = proteins, labels

    print("Data Set Size")
    print("Train set: \t\t{}".format(X_train.shape))
    
    # Create a new directory for saving logs and the final model
    subdir = datetime.datetime.strftime(datetime.datetime.now(), '%Y%m%d-%H%M%S')
    model_dir = "./" + subdir
    os.mkdir(model_dir)

    with open(model_dir + '/log_' + subdir + '.txt',"w+") as log:
        # Create a new model architecture
        model_name = "Unidirectional_LSTM-proteins_fusion with Dropout"

        # Train the new model with CV
        model = model() 

        # Save the model
        tf.keras.models.save_model(model, model_dir + '/model_' + subdir + '.hdf5')
 

def new_model_cross_validation(x_train, y_train, log_file, vocab_to_int, model_name,
          proteins, labels, seed=3, n_epochs=30, batch_size=150):
    """
    Train the network
    :param x_train:
    :param y_train:
    :param x_val:
    :param y_val:
    :return:
    """
    # Stop fitting the model if val_loss does not increase for 5 epochs
    callback = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=5)

    # Reduce learning rate when a metric has stopped improving.
    reduce_lr = tf.keras.callbacks.ReduceLROnPlateau(monitor='val_loss', factor=0.75,
                            verbose=1, patience=3, min_lr=0.0001)

    # Train by using cross-validation
    kfold = StratifiedKFold(n_splits=3, 
                            shuffle=True, 
                            random_state=seed)
    cvscores = []

    utl.print_and_write(log_file, "*** TRAINING PHASE ***\n")

    # Print infos about training (n_epochs_batch_size, etc.)
    utl.print_and_write(log_file, "Number of epochs: {}\n".format(n_epochs))
    utl.print_and_write(log_file, "Batch size: {}\n".format(batch_size))

    counter = 1
    for train, test in kfold.split(x_train, y_train):
        net = model(len(vocab_to_int) + 1, log_file, model_name)

        # Fit the model
        net.model.fit(x_train[train], y_train[train], epochs=n_epochs, 
                batch_size=batch_size, verbose=1,
                validation_data=(x_train[test], y_train[test]))                        
        
        # Evaluate the model
        scores = net.evaluate(x_train[test], y_train[test], verbose=1)
        # print("%s: %.2f%%" % (model.metrics_names[1], scores[1]*100))
        utl.print_and_write(log_file, "Fold %d " % counter)
        utl.print_and_write(log_file, "%s: %.2f%%\n" % (model.metrics_names[1], scores[1]*100))
        cvscores.append(scores[1] * 100)
        counter += 1

    # print("%.2f%% (+/- %.2f%%)" % (np.mean(cvscores), np.std(cvscores)))        
    utl.print_and_write(log_file, "Mean accuracy: %.2f%% (+/- %.2f%%)\n\n" % (np.mean(cvscores), np.std(cvscores))) 

        # Retrain the model by using the train set
    net.model.fit(proteins, labels, epochs=30, batch_size=150, verbose=1)

    return model 

def read_data(dataset_name):  
  data = pd.read_csv(dataset_name)

  # Perform a shuffle
  data.sample(frac=1, random_state=2)

  features = data.protein.values
  labels = data.label.map({'C': 1,  'N': 0}).values

  return features, labels

def plot_history(history):
    loss = history.history['loss']
    val_loss = history.history['val_loss']
    epochs = range(0, len(loss))
    plt.figure()
    plt.plot(epochs, loss, 'bo', label='Training loss')
    plt.plot(epochs, val_loss, 'b', label='Validation loss')
    plt.title('Training and validation loss')
    plt.legend()
    plt.savefig('loss.png')
  

if __name__ == "__main__":
    main()